{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64acca5-296c-4ec9-97e5-d5d8c7b24d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0addcdd1e607413abb755b24c8265f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#log into huggingface for api access\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90499f45-78f2-453b-a3b8-65abd637e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "#for prompt handling\n",
    "import json\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2515f653-bce1-4cdc-a211-92b9d5c91cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alcha\\anaconda3\\envs\\llama2_7b_workspace\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:712: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\alcha\\anaconda3\\envs\\llama2_7b_workspace\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f31b4d700d440ebf98898b7590a67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "#initiate model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                          use_auth_token=True,)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float32,\n",
    "                                             #torch_dtype=torch.float16,\n",
    "                                             use_auth_token=True,\n",
    "                                            #  load_in_8bit=True,\n",
    "                                            #  load_in_4bit=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a66a16d-c8ec-43ad-a235-4ba645260332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are an AI assistant. Always answer as helpfully as possible. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=512,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text['text'], width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22431965-22f2-4bfa-9e00-c0c0c25e13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dab8830-6715-4138-aeaf-c9392dcce93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c6c3f1-fb52-4502-a95c-45517addc7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are an advanced assistant that rates feedback and workplace appropriateness out of ten. \n",
      "<</SYS>>\n",
      "\n",
      "Provide  Workplace appropriateness and overall rating for the feedback: {text}[/INST]\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are an advanced assistant that rates feedback and workplace appropriateness out of ten. \"\n",
    "instruction = \"Provide  Workplace appropriateness and overall rating for the feedback: {text}\"\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c83e5b2-d133-484e-a43c-26d743dbc262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '  Workplace appropriateness: 9/10\\n\\nOverall rating: 9/10\\n\\nThe feedback provided is generally appropriate and professional for a workplace setting. It highlights James\\' exceptional performance and professionalism, which is a valuable asset to any organization. However, there are a few minor areas where the language could be adjusted to be even more workplace-appropriate:\\n\\n* Instead of using the phrase \"delivers exceptional results,\" consider using a more neutral term such as \"achieves outstanding outcomes\" or \"exhibits exceptional performance.\"\\n* The term \"attorney\" could be capitalized to reflect its proper title.\\n\\n Minor adjustments aside, the feedback provides a clear and positive endorsement of James\\' abilities and work ethic, and is overall a well-crafted and professional piece of feedback.'}\n"
     ]
    }
   ],
   "source": [
    "feedback = \"James is a highly professional attorney who consistently delivers exceptional results.\"\n",
    "output = llm_chain.invoke(feedback)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aaa7fb8-5af8-44df-9740-b0f8e2a8b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '  Workplace appropriateness: 8/10\\n\\nOverall rating: 9/10\\n\\nYour feedback is generally appropriate and respectful in a professional setting. However, I would recommend avoiding using overly casual language or expressions of excitement in a workplace setting, as it may come across as unprofessional to some colleagues or supervisors.\\n\\nHere\\'s why I gave the ratings I did:\\n\\nWorkplace appropriateness: 8/10\\n\\n* You use language that is generally appropriate for a professional setting, such as \"freaking rockstar\" and \"I love his energy and enthusiasm.\"\\n* However, I would suggest avoiding the use of overly casual language or expressions of excitement in a workplace setting, as it may come across as unprofessional to some colleagues or supervisors.\\n\\nOverall rating: 9/10\\n\\n* Your feedback is generally positive and respectful, and it provides specific examples of James\\' strengths in handling clients.\\n* However, I would suggest providing more balanced feedback, including any areas where James may need improvement. This will help him grow and develop in his role, and will also show that you are a fair and objective evaluator.\\n\\nRemember, when providing feedback in a workplace setting, it\\'s important to be professional, respectful, and constructive. By following these guidelines, you can help create a positive and supportive work environment for everyone.'}\n"
     ]
    }
   ],
   "source": [
    "feedback = \"OMG, James is absolutely amazing! He is a freaking rockstar when it comes to handling clients. I love his energy and enthusiasm.\"\n",
    "output = llm_chain.invoke(feedback)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894fa350-5029-4c6c-a7d2-04e866b1d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '  I would rate the feedback as a 7 out of 10 in terms of workplace appropriateness. While it\\'s great to express enthusiasm and appreciation for a colleague\\'s work, using language like \"bro\" and suggesting that someone should get a raise without proper channels or evidence can come across as informal and disrespectful in a professional setting. It\\'s important to maintain a professional tone and manner when providing feedback, especially when it comes to performance evaluations or requests for raises.\\n\\nOverall, I would give the feedback a rating of 7 out of 10. While it\\'s clear that the speaker has a positive opinion of James, the language and tone used could be improved to make it more appropriate for a professional workplace.'}\n"
     ]
    }
   ],
   "source": [
    "feedback = \"James is freaking amazing! I love working with James cause he is my bro. He knows his shit so he should get a raise.\"\n",
    "output = llm_chain.invoke(feedback)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "290053e4-cb97-43d1-b26b-482b87c4b782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"  As an advanced assistant, I appreciate your feedback! However, I must inform you that the feedback you provided is somewhat vague and lacks specific details. To provide a more accurate rating, I would need more context and information about the situation you are referring to.\\n\\nBased on the feedback you provided, I would give James a rating of 6 out of 10 for workplace appropriateness. The feedback is generally positive, but it does not provide any concrete examples or specific instances where James demonstrated his responsiveness or effort.\\n\\nOverall, I would give James a rating of 7 out of 10 for his feedback. While the feedback is generally positive, it could benefit from more specific examples and details to provide a clearer picture of James' performance.\"}\n"
     ]
    }
   ],
   "source": [
    "feedback = \"James tries hard and is responsive.\"\n",
    "output = llm_chain.invoke(feedback)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46762d-81e5-48bc-ac23-ada43cf54920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
